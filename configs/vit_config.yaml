# Vision Transformer Configuration
model:
  name: "Vision Transformer"
  type: "vit"
  description: "Transformer architecture for images (2020)"
  pretrained: true
  num_classes: 4
  variant: "vit_base_patch16_224"  # hoáº·c vit_small, vit_large

training:
  num_epochs: 15
  batch_size: 16  # Smaller for ViT
  learning_rate: 0.00001  # Very low for ViT
  weight_decay: 0.0001
  
  optimizer:
    type: "AdamW"  # AdamW better for ViT
    betas: [0.9, 0.999]
  
  scheduler:
    use: true
    type: "CosineAnnealingLR"
    T_max: 15
  
  loss:
    type: "CrossEntropyLoss"

data:
  normalization: "imagenet"
  num_workers: 2
  pin_memory: true

checkpoint:
  save_dir: "checkpoints"
  save_best: true
  save_every: 3

device: "cuda"