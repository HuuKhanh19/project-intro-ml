{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Results\n",
    "## Chest X-ray Classification\n",
    "\n",
    "Visualize training curves and evaluation metrics for individual experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose experiment to visualize\n",
    "experiment_id = 'exp01_densenet121_weighted_ce'  # Change this\n",
    "\n",
    "checkpoint_dir = Path('../checkpoints') / experiment_id\n",
    "print(f\"Experiment: {experiment_id}\")\n",
    "print(f\"Directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = checkpoint_dir / 'checkpoint_best.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "history = checkpoint['history']\n",
    "best_epoch = checkpoint['best_epoch']\n",
    "best_val_acc = checkpoint['best_val_acc']\n",
    "\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"Best Val Acc: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved training curves\n",
    "curves_path = checkpoint_dir / 'training_curves.png'\n",
    "if curves_path.exists():\n",
    "    display(IPImage(filename=str(curves_path)))\n",
    "else:\n",
    "    print(\"Training curves not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "metrics_path = checkpoint_dir / 'evaluation' / 'metrics_test.json'\n",
    "\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"Test Set Metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"  Precision: {metrics['precision']*100:.2f}%\")\n",
    "    print(f\"  Recall:    {metrics['recall']*100:.2f}%\")\n",
    "    print(f\"  F1 Score:  {metrics['f1']*100:.2f}%\")\n",
    "    if metrics.get('auc'):\n",
    "        print(f\"  AUC:       {metrics['auc']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Metrics not found! Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "cm_path = checkpoint_dir / 'evaluation' / 'confusion_matrix_test.png'\n",
    "if cm_path.exists():\n",
    "    display(IPImage(filename=str(cm_path)))\n",
    "else:\n",
    "    print(\"Confusion matrix not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ROC curves\n",
    "roc_path = checkpoint_dir / 'evaluation' / 'roc_curves_test.png'\n",
    "if roc_path.exists():\n",
    "    display(IPImage(filename=str(roc_path)))\n",
    "else:\n",
    "    print(\"ROC curves not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class metrics\n",
    "if metrics_path.exists():\n",
    "    classes = ['Normal', 'Pneumonia', 'COVID', 'Tuberculosis', 'Pneumothorax']\n",
    "    \n",
    "    precision = [p * 100 for p in metrics['precision_per_class']]\n",
    "    recall = [r * 100 for r in metrics['recall_per_class']]\n",
    "    f1 = [f * 100 for f in metrics['f1_per_class']]\n",
    "    \n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, precision, width, label='Precision', color='steelblue')\n",
    "    ax.bar(x, recall, width, label='Recall', color='lightcoral')\n",
    "    ax.bar(x + width, f1, width, label='F1 Score', color='lightgreen')\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax.set_title('Per-Class Performance', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}