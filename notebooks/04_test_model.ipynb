{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39421d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from src.models import (\n",
    "    LeNet5, ResNet50, DenseNet121,\n",
    "    VisionTransformer, CLIPClassifier\n",
    ")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94bbaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. LeNet-5 (1998)\n",
      "======================================================================\n",
      "Input:  torch.Size([2, 3, 224, 224])\n",
      "Output: torch.Size([2, 4])\n",
      "Params: 5,612,216\n",
      "✅ LeNet5 OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test LeNet5\n",
    "print(\"=\" * 70)\n",
    "print(\"1. LeNet-5 (1998)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = LeNet5(num_classes=4)\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"✅ LeNet5 OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdd1841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "2. ResNet-50 (2015)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/chest-xray/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/chest-xray/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ducluong/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 27.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([2, 3, 224, 224])\n",
      "Output: torch.Size([2, 4])\n",
      "Params: 23,516,228\n",
      "✅ ResNet50 OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test ResNet50\n",
    "print(\"=\" * 70)\n",
    "print(\"2. ResNet-50 (2015)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = ResNet50(num_classes=4, pretrained=True)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"✅ ResNet50 OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9066722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "3. DenseNet-121 (2017)\n",
      "======================================================================\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/ducluong/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/chest-xray/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 30.8M/30.8M [00:01<00:00, 28.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([2, 3, 224, 224])\n",
      "Output: torch.Size([2, 4])\n",
      "Params: 6,957,956\n",
      "✅ DenseNet121 OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test DenseNet121\n",
    "print(\"=\" * 70)\n",
    "print(\"3. DenseNet-121 (2017)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = DenseNet121(num_classes=4, pretrained=True)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"✅ DenseNet121 OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e51ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "4. Vision Transformer (2020)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d131d073744f69955097808eb494c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([2, 3, 224, 224])\n",
      "Output: torch.Size([2, 4])\n",
      "Params: 85,801,732\n",
      "✅ ViT OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test Vision Transformer\n",
    "print(\"=\" * 70)\n",
    "print(\"4. Vision Transformer (2020)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = VisionTransformer(num_classes=4, pretrained=True)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"✅ ViT OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3e654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "5. CLIP - Vision Language Model (2021)\n",
      "======================================================================\n",
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1761ee3c734b27aad67aef49ea8052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6268bc46b38a4585bfddd966678557cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a0cfc6cf27486ca6d3eb96d0d20c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b2328bc8a54e4e905bc87b9c486c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac9f9727c50412d807d36e96bfb90da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb6b2d5e4724c28bdda0b10b8c5f92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c28a3f8b864d588b13e3c6b1be123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c0a3d238ce45a0a7d35aeb30866781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a673e0c6b713464fbcfbf2a68667340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CLIP loaded with 4 classes\n",
      "Predicted class: COVID-19\n",
      "Probabilities: tensor([0.0278, 0.0200, 0.0238, 0.0117])\n",
      "✅ CLIP OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test CLIP (Zero-shot)\n",
    "print(\"=\" * 70)\n",
    "print(\"5. CLIP - Vision Language Model (2021)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class_names = ['COVID-19', 'Normal', 'Pneumonia', 'Tuberculosis']\n",
    "\n",
    "clip_model = CLIPClassifier(class_names=class_names, device=device)\n",
    "\n",
    "# Test với dummy image\n",
    "dummy_pil = Image.new('RGB', (224, 224), color='gray')\n",
    "pred_idx, probs = clip_model.predict(dummy_pil)\n",
    "\n",
    "print(f\"Predicted class: {class_names[pred_idx]}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(\"✅ CLIP OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff48d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/chest-xray/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                          Parameters     Year\n",
      "----------------------------------------------------------------------\n",
      "LeNet-5 (1998)                  5,612,216     1998\n",
      "ResNet-50 (2015)               23,516,228     2015\n",
      "DenseNet-121 (2017)             6,957,956     2017\n",
      "ViT-Base (2020)                85,801,732     2020\n",
      "CLIP (zero-shot)           ~151M (frozen)     2021\n",
      "\n",
      "✅ All 5 models tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Summary comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_info = {\n",
    "    'LeNet-5 (1998)': LeNet5(4),\n",
    "    'ResNet-50 (2015)': ResNet50(4, pretrained=False),\n",
    "    'DenseNet-121 (2017)': DenseNet121(4, pretrained=False),\n",
    "    'ViT-Base (2020)': VisionTransformer(4, pretrained=False),\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<25} {'Parameters':>15} {'Year':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in models_info.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    year = name.split('(')[1].split(')')[0]\n",
    "    print(f\"{name:<25} {params:>15,} {year:>8}\")\n",
    "\n",
    "print(f\"{'CLIP (zero-shot)':<25} {'~151M (frozen)':>15} {'2021':>8}\")\n",
    "print(\"\\n✅ All 5 models tested successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest-xray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
