{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare All Models\n",
    "## Chest X-ray Classification\n",
    "\n",
    "Compare results from all 8 experiments (4 models √ó 2 loss functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary CSV (generated by compare_results.py)\n",
    "summary_path = Path('../results/summary.csv')\n",
    "\n",
    "if summary_path.exists():\n",
    "    df = pd.read_csv(summary_path)\n",
    "    print(\"Loaded results for\", len(df), \"experiments\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"Summary not found! Run: python scripts/experiments/compare_results.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by accuracy\n",
    "df_sorted = df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ MODEL RANKING BY ACCURACY:\\n\")\n",
    "for i, row in enumerate(df_sorted.itertuples(), 1):\n",
    "    print(f\"{i}. {row.model} ({row.loss}): {row.accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved comparison plots\n",
    "plots = [\n",
    "    '../results/accuracy_comparison.png',\n",
    "    '../results/metrics_comparison.png',\n",
    "    '../results/model_comparison_by_loss.png'\n",
    "]\n",
    "\n",
    "for plot_path in plots:\n",
    "    if Path(plot_path).exists():\n",
    "        print(f\"\\n{Path(plot_path).stem}:\")\n",
    "        display(IPImage(filename=plot_path))\n",
    "    else:\n",
    "        print(f\"Plot not found: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Weighted CE vs Focal Loss\n",
    "wce_mean = df[df['loss'] == 'Weighted CE']['accuracy'].mean()\n",
    "focal_mean = df[df['loss'] == 'Focal Loss']['accuracy'].mean()\n",
    "\n",
    "print(\"Average Performance by Loss Function:\")\n",
    "print(f\"  Weighted CE:   {wce_mean:.2f}%\")\n",
    "print(f\"  Focal Loss:    {focal_mean:.2f}%\")\n",
    "print(f\"  Difference:    {abs(wce_mean - focal_mean):.2f}%\")\n",
    "\n",
    "winner = \"Weighted CE\" if wce_mean > focal_mean else \"Focal Loss\"\n",
    "print(f\"\\nüèÜ Better Loss: {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model architectures (average across both losses)\n",
    "model_avg = df.groupby('model')['accuracy'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Average Performance by Model Architecture:\")\n",
    "for model, acc in model_avg.items():\n",
    "    print(f\"  {model:18s}: {acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Architecture: {model_avg.idxmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of accuracy distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df.boxplot(column='accuracy', by='loss', ax=ax)\n",
    "ax.set_xlabel('Loss Function', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Accuracy Distribution by Loss Function', fontsize=14, fontweight='bold')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best overall model\n",
    "best_row = df_sorted.iloc[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üèÜ BEST MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model:         {best_row['model']}\")\n",
    "print(f\"Loss:          {best_row['loss']}\")\n",
    "print(f\"Experiment ID: {best_row['experiment_id']}\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  Accuracy:    {best_row['accuracy']:.2f}%\")\n",
    "print(f\"  Precision:   {best_row['precision']:.2f}%\")\n",
    "print(f\"  Recall:      {best_row['recall']:.2f}%\")\n",
    "print(f\"  F1 Score:    {best_row['f1']:.2f}%\")\n",
    "if not pd.isna(best_row['auc']):\n",
    "    print(f\"  AUC:         {best_row['auc']:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export formatted table\n",
    "export_df = df_sorted[['model', 'loss', 'accuracy', 'precision', 'recall', 'f1']].copy()\n",
    "export_df.columns = ['Model', 'Loss', 'Accuracy (%)', 'Precision (%)', 'Recall (%)', 'F1 (%)']\n",
    "export_df = export_df.round(2)\n",
    "\n",
    "print(\"\\nFormatted Results Table:\")\n",
    "display(export_df)\n",
    "\n",
    "# Save to LaTeX (for paper/thesis)\n",
    "latex_path = '../results/results_table.tex'\n",
    "export_df.to_latex(latex_path, index=False)\n",
    "print(f\"\\n‚úì LaTeX table saved to: {latex_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}